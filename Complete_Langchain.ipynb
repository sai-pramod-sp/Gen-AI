{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain\n",
    "\n",
    "###### The basic building block of LangChain is a Large Language Model which takes text as input and generates more text\n",
    "###### Suppose we want to generate a company name based on the company description, so we will first initialize an OpenAI wrapper. In this case, since we want the output to be more random, we will intialize our model with high temprature.\n",
    "###### The temperature parameter adjusts the randomness of the output. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Huggingface_Key = os.getenv(\"HUGGING_FACE_API\")\n",
    "Google_Key = os.getenv(\"GOOGLE_KEY_API\")\n",
    "OpenAi_Key = os.getenv(\"OPEN_KEY_API\")\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# temperature value--> how creative we want our model to be\n",
    "#0 ---> temperature it means model is very safe it is not taking any bets.\n",
    "#1 --> it will take risk it might generate wrong output but it is very creative\n",
    "\n",
    "llm = OpenAI(openai_api_key=OpenAi_Key, temperature=0.8)\n",
    "print(llm.model_name)\n",
    "\n",
    "text = \"what would be a good company name that produces electronic gadgets\"\n",
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n",
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ich bin Pramod?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm2 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0})\n",
    "\n",
    "llm2(\"Traslate english to german: My name is pramod?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food and give me some suggestions for the restaurant name\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm3 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'], \n",
    "    template=\"I want to open a restaurant for {cuisine} food and give me some suggestions for the restaurant name\"\n",
    ")\n",
    "\n",
    "prompt = prompt_template_name.format(cuisine = \"Italian\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I am fan of Italian biryani. Suggest me a good name for my son so that by hearing that name it should remind me'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt1 = PromptTemplate.from_template(\n",
    "    \" I am fan of {product} biryani. Suggest me a good name for my son so that by hearing that name it should remind me\"\n",
    ")\n",
    "prompt1.format(product=\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m I am fan of Hyderabad biryani. Suggest me a good name for my son so that by hearing that name it should remind me\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "hyderabad\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm3, prompt=prompt1, verbose=True)  # verbose is for viewing the flow of the chain\n",
    "response = chain.run(\"Hyderabad\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0.8})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'], \n",
    "    template=\"I want to open a restaurant for {cuisine} food and give me some suggestions for the restaurant name\"\n",
    ")\n",
    "\n",
    "chain_cuisine = LLMChain(llm=llm4, prompt=prompt1, verbose=True, output_key=\"restaurant name\")\n",
    "\n",
    "prompt_template_name1 = PromptTemplate(\n",
    "    input_variables=['restaurant name'], \n",
    "    template=\"Suggest 10 menu items for {restaurant name}\"\n",
    ")\n",
    "\n",
    "chain_name = LLMChain(llm = llm4, prompt=prompt_template_name1, verbose=True, output_key=\"menu items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m I am fan of indian biryani. Suggest me a good name for my son so that by hearing that name it should remind me\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest 10 menu items for samantha\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "grilled chicken, grilled asparagus, grilled chicken, grilled chicken, grilled asparagus\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain2 = SimpleSequentialChain(\n",
    "    chains = [chain_cuisine, chain_name]\n",
    ")\n",
    "\n",
    "content = chain2.run(\"indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0.8})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm4, prompt=prompt_template_name, output_key=\"restaurant_name\", verbose=True)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm4, prompt=prompt_template_items, output_key=\"menu_items\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for italian food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some menu items for io tagliacci.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'cuisine': 'italian', 'restaurant_name': 'io tagliacci', 'menu_items': 'grilled octopus, grilled octopus, grilled '}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")\n",
    "\n",
    "output = chain({\"cuisine\":\"italian\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[indian economy, gdp]\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp]\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp]\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m[indian economy, gdp] Observation: Invalid Format: Miss\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "\n",
    "llm4 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0.8})\n",
    "\n",
    "#serpapi = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)\n",
    "\n",
    "#Google Search API\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm4)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools,\n",
    "                        llm4,\n",
    "                        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                        verbose=True,handle_parsing_errors=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of india in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "llm4 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
    "                      huggingfacehub_api_token=Huggingface_Key, \n",
    "                      model_kwargs={\"temperature\": 0.8})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm4, prompt=prompt_template_name, output_key=\"restaurant_name\", verbose=True)\n",
    "name = name_chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for indian food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Indian restaurant\n"
     ]
    }
   ],
   "source": [
    "name2 = name_chain.run(\"indian\")\n",
    "print(name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(name_chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm4, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian restaurant\n",
      "Human: Mexican\n",
      "AI: Mexican restaurant\n",
      "Human: indian\n",
      "AI: Indian restaurant\n"
     ]
    }
   ],
   "source": [
    "name2 = chain.run(\"indian\")\n",
    "print(name2)\n",
    "\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationChain\n",
    "\n",
    "###### Conversation buffer memory goes growing endlessly\n",
    "###### Just remember last 5 Conversation Chain\n",
    "###### Just remember last 10-20 Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Models\\envAI2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=llm4)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was won by India.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5+5 is a number that is repeated indefinitely.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The captain of the winning team was Virat Kohli.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI: The first cricket world cup was won by India.\n",
      "Human: How much is 5+5?\n",
      "AI: 5+5 is a number that is repeated indefinitely.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI: The captain of the winning team was Virat Kohli.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was won by India.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)  # K value is  to remember the no of contexts given by the user\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=llm4,\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5+5 is a number that is repeated indefinitely.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The captain of the winning team was Virat Kohli.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'simple.pdf', 'page': 0}, page_content='TYPEOriginalResearch\\nPUBLISHED 01 June 2023\\nDOI10.3389/fenrg.2023.1172176\\nOPENACCESS\\nEDITEDBY\\nWen Zhong Shen,\\nYangzhou University, China\\nREVIEWEDBY\\nAnitha Gopalan,\\nSaveetha University, India\\nS. Jenoris Muthiya,\\nDayananda Sagar College of\\nEngineering, India\\n*CORRESPONDENCE\\nEl-Sayed M. El-Kenawy,\\nSkenawy@ieee.org\\nRECEIVED 23 February 2023\\nACCEPTED 16 May 2023\\nPUBLISHED 01 June 2023\\nCITATION\\nAlhussan AA, M. El-Kenawy E-S,\\nAbdelhamid AA, Ibrahim A, Eid MM and\\nKhafaga DS (2023), Wind speed\\nforecasting using optimized bidirectional\\nLSTM based on dipper throated and\\ngenetic optimization algorithms.\\nFront. Energy Res. 11:1172176.\\ndoi: 10.3389/fenrg.2023.1172176\\nCOPYRIGHT\\n© 2023 Alhussan, M. El-Kenawy,\\nAbdelhamid, Ibrahim, Eid and Khafaga.\\nThis is an open-access article distributed\\nunder the terms of the Creative\\nCommons Attribution License (CC BY) .\\nThe use, distribution or reproduction in\\nother forums is permitted, provided the\\noriginal author(s) and the copyright\\nowner(s) are credited and that the\\noriginal publication in this journal is\\ncited, in accordance with accepted\\nacademic practice. No use, distribution\\nor reproduction is permitted which does\\nnot comply with these terms.Wind speed forecasting using\\noptimized bidirectional LSTM\\nbased on dipper throated and\\ngenetic optimization algorithms\\nAmel Ali Alhussan1, El-Sayed M. El-Kenawy2*,\\nAbdelaziz A. Abdelhamid3,4, Abdelhameed Ibrahim5,\\nMarwa M. Eid6and Doaa Sami Khafaga1\\n1Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah\\nBint Abdulrahman University, Riyadh, Saudi Arabia,2Department of Communications and Electronics,\\nDelta Higher Institute of Engineering and Technology, Mansoura, Egypt,3Department of Computer\\nScience, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt,\\n4Department of Computer Science, College of Computing and Information Technology, Shaqra\\nUniversity, Shaqra, Saudi Arabia,5Computer Engineering Department, College of Engineering and\\nComputer Science, Mustaqbal University, Buraydah, Saudi Arabia,6Faculty of Artificial Intelligence,\\nDelta University for Science and Technology, Mansoura, Egypt\\nAccurate forecasting of wind speed is crucial for power systems stability.\\nMany machine learning models have been developed to forecast wind\\nspeed accurately. However, the accuracy of these models still needs more\\nimprovements to achieve more accurate results. In this paper, an optimized\\nmodel is proposed for boosting the accuracy of the prediction accuracy of\\nwind speed. The optimization is performed in terms of a new optimization\\nalgorithm based on dipper-throated optimization (DTO) and genetic algorithm\\n(GA), which is referred to as (GADTO). The proposed optimization algorithm is\\nused to optimizethe bidrectionallong short-term memory (BiLSTM) forecasting\\nmodel parameters. To verify the effectiveness of the proposed methodology,\\na benchmark dataset freely available on Kaggle is employed in the conducted\\nexperiments. The dataset is first preprocessed to be prepared for further\\nprocessing. In addition, feature selection is applied to select the significant\\nfeatures in the dataset using the binary version of the proposed GADTO\\nalgorithm. The selected features are utilized to learn the optimization algorithm\\ntoselectthebestconfigurationoftheBiLSTMforecastingmodel.Theoptimized\\nBiLSTM is used to predict the future values of the wind speed, and the resulting\\npredictions are analyzed using a set of evaluation criteria. Moreover, a statistical\\ntest is performed to study the statistical difference of the proposed approach\\ncomparedto other approachesin termsofthe analysis ofvariance(ANOVA)and\\nWilcoxon signed-rank tests. The results of these tests confirmed the proposed\\napproach’sstatisticaldifferenceanditsrobustnessinforecastingthewindspeed\\nwith an average root mean square error (RMSE) of 0.00046, which outperforms\\nthe performance of the other recent methods.\\nKEYWORDS\\nbidirectional long short-term memory, wind speed forecasting, metaheuristic\\noptimization,dipperthroatedoptimization,geneticalgorithm,machinelearning\\nFrontiers in Energy Research 01 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 1}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\n1 Introduction\\nIncreasing reliance on renewable energy supplies directly results\\nfrom the need to meet rising energy demands while mitigating\\nthe negative environmental impacts of traditional fossil fuels.\\nRenewable energy options are vital to lessen this impact and lessen\\nrelianceonconventionalfuels.Inparticular,nationsthathavesigned\\nthe Paris Climate Agreement have committed to lowering their\\nemissions of greenhouse gases. Due to its accessibility and lack\\nof emissions, wind power is gaining popularity. The potential of\\nwind energy has been widely recognized in the previous 2\\xa0decades.\\nAs the cost to maintain and operate wind turbines drops and\\ntheir dependability improves, the number of wind farms grows\\nexponentially. Yet, the reliability of wind farms is impacted by the\\nintermittent and variable nature of wind energy ( Albalawi\\xa0et\\xa0al.,\\n2022;Chen\\xa0et\\xa0al., 2022 ). Therefore, it is becoming increasingly\\nimportant that wind energy be predictable ( Zeng\\xa0et\\xa0al., 2020 ).\\nThere has been a surge in a study on predicting wind speeds\\nin the past decade. Physical, statistical, machine learning, and\\nhybrid forecasting methods are the most common approaches\\n(Shang\\xa0et\\xa0al., 2022 ). To generate mathematical equations, physical\\napproaches draw on concepts from geophysical fluid dynamics\\nand thermodynamics. The NWP (numerical weather prediction\\nsystem) model underpins most physical approaches. The intricate\\nmathematical design of NPW models makes them extremely time-\\nconsumingtocompute( Carvalho\\xa0et\\xa0al.,2012 ).Becauseofthis,NPW\\nmodels are often reserved for long-term forecasting, making them\\nimpractical for short-term wind power predictions.\\nStatisticalmodelsassumealinearrelationshipbetweenobserved\\nwind data and wind speed to predict wind speed. While many\\nstatistical models can account for linear trends in wind speed\\nand direction, very few can account for the data’s nonlinear\\ncharacteristics ( Fang and Chiang, 2016 ;Khafaga\\xa0et\\xa0al., 2022 ).\\nThe capacity of artificial intelligence (AI) models to understand\\nthe input-output connection from historical data has led to\\ntheir increased adoption in wind forecasting. In particular, AI\\nalgorithms decipher nonlinear relationships in the data and\\nspot previously concealed patterns, allowing for accurate value\\npredictions ( Doaa\\xa0et\\xa0al., 2022 ;Sun and Jin, 2022 ). The capacity\\nto update the model in response to new information is another\\nbenefit of AI-based models ( Demolli\\xa0et\\xa0al., 2019 ). While there are\\nbenefits to using a variety of AI techniques, there are also drawbacks\\nto consider ( Shang\\xa0et\\xa0al., 2022 ). Consequently, the advantages of\\nseveral approaches are utilized in hybrid or combination models.\\nThe structure of a renewable energy management system is\\nshown in Figure\\xa01 (Fathima and Palanisamy, 2016 ). To aid in\\nthe routine upkeep and emergency repairs of electrical equipment\\nin a manufacturing facility, farm, or even an entire municipality,\\nan EMS is in place to handle energy control, management,\\nmaintenance, and consumption concerns. It can keep tabs on the\\nmachinery’s working condition and promptly boost management.\\nFor managers of an electrical company, money can be saved and\\nused to increase equipment life by practicing good management.\\nThe system may promptly send out an alarm to aid management\\npersonnel in monitoring and repair, reducing losses to a minimum\\nin the case of equipment breakdowns or other situations. The\\nEMS may also message the appropriate people when it detects\\nthat specific high-energy-use gadgets are getting on in years.Data collection, storage, processing, statistics, query, and analysis,\\nas well as data monitoring and diagnostics, are all possible in\\na renewable energy management system (REMS) thanks to the\\nemployment of programmed control system technology, network\\ncommunication, and database technology. Figure\\xa01 shows how\\nthe EMS accomplishes its monitoring and management aims\\nby dispatching electricity generated by renewable sources in\\naccordance with a projection of that power made using a forecasting\\nmodel. Per-unit energy consumption and economic and energy\\nefficiency are lowered through centralized monitoring and efficient\\nadministration of energy data. In light of this, the model used for\\nmaking predictions is a vital EMS input.\\nPhysical,statistical,machinelearning,andhybridmodelsarethe\\nmost common categories used to categorize methods for predicting\\nwind speed ( Shang\\xa0et\\xa0al., 2022 ). To predict wind speed, physical\\nmodels often use data about the weather, including meteorological\\ncharacteristics and geographical information. Regarding general-\\npurpose physical models, the numerical weather prediction (NWP)\\napproach is among the most well-known and successful ones. Using\\nthe widely-used weather research and forecast (WRF) model, the\\nauthors of ( Carvalho\\xa0et\\xa0al., 2012 ) evaluate several computational\\nand physical approaches. To provide more accurate estimates of\\nwind speeds at the Earth’s surface, the authors of ( Hoolohan\\xa0et\\xa0al.,\\n2018) integrate the NWP with the Gaussian process regression. The\\nprimarychallengesofNWPapproachesarethecomputingdemands\\nand the update frequency of forecasts.\\nThe prediction of near-term wind speeds relies heavily on\\nstatistical approaches. In their ground-breaking research, authors\\n(Brown\\xa0et\\xa0al.,1984 )predictwindspeedusinganautoregressive(AR)\\nmodel. According to ( Torres\\xa0et\\xa0al., 2005 ), the authors indicate the\\nmean hourly wind speed 10\\xa0h into the future using the ARMA\\n(autoregressive moving average process) approach. The authors\\nof (Rajagopalan and Santoso, 2009 ) use this technique similarly\\nto predict wind speed. Their findings suggest their algorithm\\ncan reliably estimate speeds within 1\\xa0h. For predicting the next\\nhour’s wind speed, the authors of ( Sfetsos, 2002 ) offer an ARIMA\\n(autoregressive integrated moving average) model that considers\\naverages over the previous 10\\xa0minutes and hour. Based on the\\ndata, it appears that 10-min\\xa0averages are more reliable. To forecast\\nwind speed up to 2\\xa0days in advance, the authors of ( Kavasseri\\nand Seetharaman, 2009 ) investigate fractional-ARIMA models.\\nWind power generation forecasting is much easier with ARIMA,\\nas demonstrated by the authors of ( Eldali\\xa0et\\xa0al., 2016 ). Using a\\ncombination of the ARIMA and clustering approaches, the authors\\nof (Akhil\\xa0et\\xa0al., 2022 ) offer a model for predicting wind speed\\na full year in advance. Short-term predictions of offshore wind\\nspeed are shown in ( Liu\\xa0et\\xa0al., 2021 ), which presents a seasonal\\nARIMA model. They evaluate how well-known machine learning\\nmethods like the GTU (gated recurrent unit) and the LSTM\\n(long short-term memory) perform in comparison to the ARIMA\\nmodel (long short-term memory). Their findings suggest that the\\nseasonal ARIMA model performs better than the GTU and the\\nLSTM. Univariate ARIMA is compared with NARX (nonlinear\\nautoregressive exogenous) models by the authors of ( Cadenas\\xa0et\\xa0al.,\\n2016). The findings demonstrate that the NARX performs better\\nwind speed prediction than the ARIMA.\\nNonlinearity in wind data has increased interest in using\\nartificial intelligence systems for predicting wind speeds. For\\nFrontiers in Energy Research 02 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 2}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nFIGURE1\\nThe typical architecture of wind speed prediction and power generation system.\\nnonlinear data, in particular, ANN (artificial neural network) is an\\neffective technique ( Sun and Jin, 2022 ). The authors of ( Cadenas\\nand Rivera, 2009 ) study short-term wind speed predictions using\\nANN models. Two-layer ANN appears to perform best in both the\\nlearningandpredictionphases.Theauthorsof( DumitruandGligor,\\n2017) use a FANN (feedforward neural networks) model to predict\\ndaily wind features from historical data. In ( Higashiyama\\xa0et\\xa0al.,\\n2017), the authors use CNN (convolutional neural networks) to\\nprocess high-dimensional data for wind forecasting. They suggest\\na feature extraction technique based on convolutional neural\\nnetworks to reduce the size of the massive datasets generated\\nby NWP. A further investigation ( Yu\\xa0et\\xa0al., 2019 ) uses CNN to\\npredict wind power generation, this time factoring in the wind’s\\ntemporal and geographical variations. For their short-term forecast\\nof Estonia’s wind energy output, the authors of ( Shabbir\\xa0et\\xa0al.,\\n2022) use a recurrent neural network (RNN) algorithm (LSTM).\\nAccordingtotheirfindings,LSTMoutperformsSVR(supportvector\\nmachines) and NAR (neighbor-association recognition) (Nonlinear\\nAutoregressive Neural Networks).\\nFor more precise and efficient wind speed predictions, hybrid\\nand combination models incorporate the best features of many\\nmodels. As an example, the authors of ( Li\\xa0et\\xa0al., 2018 ) use the\\nwavelet transform in predicting to filter out high-frequency data\\nand SVR. To improve the precision of wind power forecasting, the\\nauthors of ( Liu\\xa0et\\xa0al., 2018 ) suggest a two-stage approach. First,\\nWPD splits wind speed time data into sublayers (Wavelet Packet\\nDecomposition). They use convolutional neural networks (CNNs)\\nandCNNLSTMs(convolutionallongshort-termmemorynetworks)\\nto create predictions at both high- and low-frequency layers. Hybrid\\nmodelsareatypeofmodelinwhichanANNmodel’sinputvariables\\nand problem parameters are determined using a different approach.\\nIn(SunandJin,2022 ),forinstance,ARIMAisusedtoidentifyANN\\nmodel input neurons. In ( López and Arboleya, 2022 ), the authors\\nselecttheirinputvariablesusingthePearsonCorrelationCoefficient\\n(PCC). The authors suggest utilizing these parameters as inputs for\\nLSTM and DNN (Dynamic Neural Networks) models. The authors\\nof (Xiong\\xa0et\\xa0al., 2022 ) use the attention mechanism to prioritize\\ninput variables. Data decomposition into subseries is a relatively\\nnew method used in hybrid models. For instance, the authors’\\ndecomposition of wind speed data into subseries is performed\\nusing a wavelet technique in ( Yu\\xa0et\\xa0al., 2018 ). The RNN (recurrent\\nneural networks) and its variations LSTM and GTU are used to\\nextract more nuanced information from low-frequency subseriesfor forecasting. They found that decomposition, in addition to\\nusing hybrid models, improves the reliability of predictions. In\\n(Shang\\xa0et\\xa0al., 2022 ), the authors deconstruct historical wind speed\\ndata using CEEMD (complementary ensemble empirical mode\\ndecomposition). Then they use SOM (subspace-oriented metric)\\nclustering to organize the resulting data (self-organizing map). To\\nbetter predict how much energy will be generated by the wind, the\\nauthorsof( PraveenaandDhanalakshmi,2018 )useaFuzzyK-Means\\napproach to group together comparable days.\\nAs the most dominant approach used in time series forecasting\\nis the bidirectional long short-term memory (BiLSTM), as it\\ngives promising prediction results, it is noted that this approach\\nneeds further improvements to boost its performance. This gap\\nin the performance of the BiLSTM prediction model forms\\nthe main motivation for this research. In this research, a new\\nmetaheuristic optimization algorithm is proposed for feature\\nselection and for optimizing the parameters of BiLSTM to boost\\nits performance. The feature selection is performed using a new\\nbinary algorithm based on hybrid genetic and dipper-throated\\noptimization algorithms. In addition, the continuous version of\\nthis algorithm is used to optimize the parameters of the BiLSTM.\\nThe proposed model is capable of capturing the data’s long-term\\nvariations. The results are assessed using a set of evaluation metrics\\nand a set of statistical tests to confirm its superiority, effectiveness,\\nand statistical difference when compared to other competing\\nmodels. The main contributions of this work are listed in the\\nfollowing:\\n•A new optimization algorithm is proposed for optimizing the\\nparameters of BiLSTM.\\n•A new feature selection algorithm is proposed to select the best\\nfeatures for improving wind speed prediction.\\n•A comparison between the feature selection results is\\nperformed in terms of the proposed feature selection algorithm\\nand the other eight feature selection algorithm.\\n•A comparison between the results of wind speed prediction\\nusing BiLSTM when optimized using the proposed GADTO\\nalgorithm and four other optimization algorithms.\\n•Statistical analysis of the results is presented and discussed to\\nshow the significant difference between the proposed and other\\nmethods.\\nFrontiers in Energy Research 03 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 3}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nFIGURE2\\nThe architecture of the proposed methodology.\\nThe structure of this paper is organized as follows. In Section\\xa02 ,\\nthe proposed method is presented and discussed. Section\\xa03 explains\\nthedetailsoftheexperimentalsetupandthecomputationalfindings.\\nSection\\xa04 concludes with a brief discussion of the results and\\nsuggestions of this work for further study.\\n2 The proposed methodology\\nThe proposed wind forecasting methodology is shown in the\\nflowchart depicted in Figure\\xa02 . In this figure, there are five key\\nphases. The first phase is data preprocessing, in which the dataset\\nis collected and preprocessed to remove outliers and handle missing\\nvalues, and the data is cleaned. In addition, this phase includes data\\nnormalization that helps to eliminate the potential for bias toward\\noutlying numbers. The second phase is feature selection, in which a\\nnovel feature selection algorithm is proposed based on the dipper\\nthroated and genetic optimization algorithms. The third phase isthe optimization of the long short-term memory prediction model.\\nThe optimization of this model is applied in terms of the proposed\\noptimization algorithm. The fourth stage is predicting the wind\\nspeed using the optimized model. The fifth stage is the evaluation\\nand statistical analysis of the achieved prediction results.\\n2.1 Description of the dataset\\nThe benchmark dataset employed in this work is freely available\\nonKaggle( Fedesoriano,2022 ).Thisdatasetcontains6,574instances\\nin a collection of responses from a set of sensors within a weather\\nstation that measures five weather variables. The device was put at\\n21M in a vacant part of the wind farm. From the beginning of 1961\\ntotheendof1978,thedatawerecollectedfor17\\xa0years.Precipitation,\\nhigh and low temperatures, and the grass’s lowest temperature were\\nsupplieddailyasrecordedbytheGroundTruth. Table\\xa01 presentsthe\\ndataset’s variables and the corresponding description. In addition,\\nFrontiers in Energy Research 04 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 4}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nTABLE1 Thevariablesofthewindspeeddatasetemployedinthisresearch.\\nVariable Description\\nIND First indicator value\\nIND.1 Second indicator value\\nIND.2 Third indicator value\\nRAIN Precipitation Amount (mm)\\nT.MIN Minimum Temperature (°C)\\nT.MIN.G 09utc Grass Minimum Temperature (°C)\\nT.MAX Maximum Temperature (°C)\\nWIND Average wind speed [knots]\\nthe behavior of the variables is shown in the plots of Figure\\xa03 ,\\nand the histogram of these variables is depicted in Figure\\xa04 .\\nThese plots give insights into the nature of the variables, which\\nnecessitates a robust forecasting model to achieve high prediction\\nresults.2.2 Data preprocessing\\nObtaining a reliable prediction model requires preprocessing of\\nthe raw data. Removing anomalies from the data, such as outliers\\nand missing values, is necessary. Interpolation of the observed data\\nis used for imputation in time series to replace missing values.\\nIn addition, scaling the data into the interval [0, 1] is achieved\\nusing min-max\\xa0normalization. The historical data is then split into\\ntraining, validation, and testing sets. The machine learning model is\\nconstructed from the training set, with already known inputs and\\noutputs. The model’s hyperparameters are tuned with the help of\\nthe validation set. The testing set is used for estimating how well a\\nmodel will do with data that was not used to train it. The training set\\naccounts for 70% of the dataset, the validation set accounts for 10%,\\nand the test set accounts for 20%.\\n2.3 Long short-term memory (LSTM)\\nDue to its outstanding potential to preserve sequence\\ninformation across time, long short-term memory (LSTM) is a\\nFIGURE3\\nThe time series of the datset variables in the adopted dataset.\\nFrontiers in Energy Research 05 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 5}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nFIGURE4\\nHistogram of the dataset variables employed in this research.\\nFIGURE5\\nThe typical architecture of LSTM prediction model.\\ncomplicated computing unit that can achieve better results in\\nsequence modeling applications results from applying LSTM to\\na wide range of domains show that it can define data connected with\\ntime series dependencies and capture the data variance pattern. The\\nbursting and disappearing gradient problem in recurrent neural\\nnetwork (RNN) training may be dealt with by using LSTn. This\\nnew sort of neural network was developed to handle the challenges\\nof RNNs in learning these types of long-term incompatibilities.\\nThe strongest method for recognizing and altering the long-range\\ncontext is the LSTM architecture’s built-in memory cells. Three\\nprimary gates comprise an LSTM block: the input gate, the forget\\ngate, and the output gate with a memory cell. The cellular states are\\ncontrolled by gates like these and the sigmoid activation function.\\nInFigure\\xa05 , the LSTM cell layout is shown, which includes the\\nLSTM’s essential components, such as the added element level and\\nthemultiplicationsymbol,whichstandsforthemultiplicationofthe\\nelement levels ( Abdel\\xa0Samee\\xa0et\\xa0al., 2022 ;El-Kenawy\\xa0et\\xa0al., 2022 ).\\nThe input gate determines the magnitude of the values\\nentering the cell and being stored in the processor’s memory\\n(it). The information in a memory cell is pruned down to only\\nthe necessary bits using a mechanism called a forget gate (ft),which also determines the percentage of the original values to\\nbe retained. The LSTM’s output is activated by the output gate\\n(ot), and the activated information defines its output. Furthermore,\\nthe input node (gt)acts as an activation vector for cells. The\\nmathematicalperformanceoftheLSTMisdepictedbythefollowing\\nequations, where htis the hidden variable, and sigma is the logistic\\nsigmoid.\\nf(t)=σ(Wxfxt+Whfht−1+Wcfct−1+bf) (1)\\ni(t)=σ(Wxixt+Whiht−1+Wcict−1+bi) (2)\\ng(t)=ftgt−1+it\\u2061tanh(Wxgxt+Whght−1+bg) (3)\\no(t)=σ(Wxoxt+Whoht−1+Wcoct+bo) (4)\\nh(t)=ot\\u2061tanh(gt) (5)\\n2.4 Bidirectional long short-term memory\\n(BiLSTM)\\nTo improve the efficiency of the classification procedure, LSTM\\nis used to develop BiLSTM. As shown in Figure\\xa06 , BiLSTM is\\ndeveloped by fusing RNNs with LSTM methods (which provide\\naccesstoabroadcontext).ComparedtotraditionalRNNsandtime-\\nwindowed multilayer perceptrons, BiLSTM networks are quicker\\nand more accurate and beat unidirectional networks like LSTM.\\nIn addition, BiLSTM includes extensive information for all phases\\nbefore and after each step in the specified sequence. Moreover,\\nthe LSTM technique calculates the hidden layers in BiLSTM.\\nOne distinguishing feature of BiLSTM over LSTM is its ability\\nto process input in both directions, thanks to using two hidden\\nlayers that feed their respective outputs into the same output layer\\n(Saeed\\xa0et\\xa0al., 2020 ;Sami\\xa0Khafaga\\xa0et\\xa0al., 2022a ;Sami\\xa0Khafaga\\xa0et\\xa0al.,\\n2022b).\\nFrontiers in Energy Research 06 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 6}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nFIGURE6\\nThe typical architecture of BiLSTM prediction model.\\n2.5 Genetic algorithm\\nThe genetic algorithm (GA) optimization is inspired by the\\ngenomics operations based on the crossover and mutation process.\\nIn this work, the genetic algorithm is hybridized with the DTO\\nalgorithmtoformtheproposedGADTOalgorithmwhichisusedin\\nfeatureselectionandparametersoptimizationofthelongshort-term\\nmemory employed for predicting the wind speed values. One of the\\nmain steps of the genetic algorithm is the mutation operator, which\\nproduces a new solution with characteristics that differ from those\\nof the parent’s chromosomes. This provides many different solutions\\nrather than just one ideal solution. In this work, we developed a new\\nmutation method for improving the search space exploration. The\\nmutation operation is applied to the positions of the birds of the\\nDTO algorithm to enable them to explore more areas in the search\\nspace. The mutation is performed using the following equation.\\nP(t+1)=2⋆k⋆z2−h⋆P(t)⋆cos(P(t))\\n1−cos(P(t)(6)\\nwhereP(t)andP(t+1)denote the individual position at iteration t\\nandt+1, respectively. kis a random number in the range [0,2],zis\\na random number in the range of [0,1]with exponential behavior,\\nandhis a random number in the range ( Abdel\\xa0Samee\\xa0et\\xa0al., 2022 ;\\nAkhil\\xa0et\\xa0al., 2022 ).\\n2.6 Dipper throated optimization\\nThe optimization process of the dipper throated optimization\\n(DTO) is based on two types of groups of birds, namely, swimming\\nbirds and flying birds. These birds are looking for food, so they\\nupdate their positions (P) and velocities (V) to reach the food\\nefficiently.Thefollowingmatricesrepresentthelocationandvelocity\\nof the birds.\\nP=[[[[[[[[[[\\n[P1,1P1,2P1,3…P1,d\\nP2,1P2,2P2,3…P2,d\\nP3,1P3,2P3,3…P3,d\\n… … … … …\\nPm,1Pm,2Pm,3…Pm,d]]]]]]]]]]\\n](7)V=[[[[[[[[[[\\n[V1,1V1,2V1,3…V1,d\\nV2,1V2,2V2,3…V2,d\\nV3,1V3,2V3,3…V3,d\\n… … … … …\\nVm,1Vm,2Vm,3…Vm,d]]]]]]]]]]\\n](8)\\nWherePi,j, refers to the position of the ithbird in the jth\\ndimension for i∈[1,2,3,…,m]andj∈[1,2,3,…,d], and its velocity\\nin thejthdimension is indicated by Vi,j. For each bird, the values\\nof the fitness functions f=f1,f2,f3,…,fnare determined by the\\nfollowing matrix.\\nf=[[[[[[[[[[\\n[f1(P1,1,P1,2,P1,3,…,P1,d)\\nf2(P2,1,P2,2,P2,3,…,P2,d)\\nf3(P3,1,P3,2,P3,3,…,P3,d)\\n…\\nfm(Pm,1,Pm,2,Pm,3,…,Pm,d)]]]]]]]]]]\\n](9)\\n2.7 The proposed optimization algorithm\\nThe steps of the proposed optimization algorithm are presented\\ninAlgorithm\\xa01 . In these steps, both DTO and GA algorithms are\\nhybridized in a unified algorithm in which the dynamic swapping\\nbetween the DTO and GA algorithms improves the search space\\nexploration. This algorithm benefits from the mutation step of\\nthe GA algorithm to help birds of the DTO algorithm better\\nexplore the search space and thus can find the best solution more\\naccurately. In this algorithm, tandTmaxrefer to the iteration\\nnumber and the maximum number of iterations, respectively. The\\nvalues of the parameters r1,r2,r3,K,K1,K2,K3,K4,K5,zare selected\\nrandomly. PbestandPGbestrefer to the local and global best\\nsolutions.\\n2.8 Feature selection\\nThe feature selection process eliminates excessive, redundant,\\nand noisy data. The main benefit of the feature selection is that\\nit helps improve the model’s performance as it decreases the\\ndimensionality of the dataset. Since employing raw features might\\nproduceineffectiveresults,optimalfeatureselectioncanbeessential\\nin providing precise forecasts. Consequently, several techniques\\nemployed various feature selection strategies before using the data\\nto train a model ( El-kenawy\\xa0et\\xa0al., 2022 ). Relevant features are\\nselected from raw data using the binary version of the proposed\\noptimizationalgorithm,whichisdescribedbythestepspresentedin\\nAlgorithm\\xa02 .\\n3 Experimental results\\nThe conducted experiments are classified into two types. The\\nfirst type is a set of experiments targeting and evaluating the\\nFrontiers in Energy Research 07 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 7}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\n1:Initialize birds’ positions Pi(i=1,2,…,n)forn\\nbirds, birds’ velocity Vi(i=1,2,…,n), objective\\nfunction fn, iterations t,Tmax, parameters of\\nr1,r2,r3,K,K1,K2,K3,K4,K5,z\\n2:Calculate fitness of fnfor each bird Pi\\n3:Findbest bird position Pbest\\n4:Convert best solution to binary [0,1]\\n5:Sett=1\\n6:whilet≤Tmaxdo\\n7:\\xa0for(i=1:i<n+1)do\\n8:\\xa0\\xa0if(t{%}2==0)then\\n9:\\xa0\\xa0\\xa0if(r3<0.5)then\\n10:\\xa0\\xa0\\xa0\\xa0Updatethe current swimming bird’s position\\nas:P(i+1)=Pbest(i)−K1.|K2.Pbest(i)−P(i)|\\n11:\\xa0\\xa0\\xa0else\\n12:\\xa0\\xa0\\xa0\\xa0Updatethe current flying bird’s velocity\\nas:\\nV(i+1)=K3V(i)+K4r1(Pbest(i)−P(i))+K5r2(PGbest−P(i))\\n13:\\xa0\\xa0\\xa0\\xa0Updatethe current flying bird’s position\\nas:P(i+1)=P(i)+V(i+1)\\n14:\\xa0\\xa0\\xa0end if\\n15:\\xa0\\xa0else\\n16:\\xa0\\xa0\\xa0Mutatebirds’ positions using:\\nP(i+1)=2⋆K⋆z2−h⋆P(i)⋆cos(P(i))\\n1−cos(P(i)\\n17:\\xa0\\xa0end if\\n18:\\xa0end for\\n19:\\xa0Updater1,r2,r3,K,K1,K2,K3,K4,K5,z\\n20:\\xa0Calculate objective function fnfor each bird\\nPi\\n21:\\xa0Findthe best position Pbest\\n22:end while\\n23:Returnthe best solution PGbest\\nAlgorithm1.TheproposedGADTOalgorithm.\\nproposed feature selection algorithm. Whereas the second type is a\\nset of experiments that assessed the optimized LSTM model, which\\nis optimized using the proposed GADTO algorithm. This section\\npresentstheresultsachievedbythesesetsofexperimentsinaddition\\nto the statistical analysis of these results.\\n3.1 Configuration parameters of\\noptimization algorithms\\nThe conducted experiments include a set of optimization\\nalgorithms; namely, standard GA ( Immanuel and Chakraborty,\\n2019) standard DTO ( Takieldeen\\xa0et\\xa0al., 2022 ), particle\\nswarm optimization (PSO) ( Awange\\xa0et\\xa0al., 2018 ), grey wolf\\noptimization (GWO) ( Mirjalili\\xa0et\\xa0al., 2014 ), whale optimization\\nalgorithm (WOA) ( Mirjalili and Lewis, 2016 ) and firefly\\nalgorithm (FA) ( Ariyaratne and Fernando, 2023 ). The basic\\nconfiguration parameters of these algorithms are presented in\\nTable\\xa02 .1:Initialize the parameters of GADTO algorithm\\n2:Convert the resulting best solution to binary\\n[0,1]\\n3:Evaluate the fitness of the resulting\\nsolutions\\n4:TrainKNN to assess the resulting solutions\\n5:Sett=1\\n6:whilet≤Maxiterationdo\\n7:\\xa0RunGADTO algorithm to get best solutions\\nPbest\\n8:\\xa0Convert best solutions to binary using the\\nfollowing equation:\\nBinary_Solution ={{\\n{{\\n{1 ifF(Pbest)≥0.5\\n0otherwise,\\nF(Pbest)=1\\n1+e−10(Pbest−0.5)\\n9:\\xa0Calculate the fitness value\\n10:\\xa0Updatethe parameters of GADTO algorithm\\n11:\\xa0Updatet = t + 1\\n12:end while\\n13:Returnbest set of features\\nAlgorithm2.TheproposedbinaryGADTO(bGADTO)algorithm.\\n3.2 Evaluation metrics\\nThe evaluation metrics of the results achieved by the conducted\\nexperiments are categorized into two sets of metrics. The first\\nset of metrics is presented in Table\\xa03 and used to assess the\\nfeature selection results. This set of metrics includes best fitness,\\nworst fitness, average error, average fitness, average fitness size, and\\nstandarddeviation.Thesecondsetofmetricsispresentedin Table\\xa04 ,\\nwhich includes mean bias error (MBE), root mean square error\\n(RMSE), mean absolute percentage error (MAPE), mean absolute\\nerror (MAE), R-squared (R2), Willmott’s Index (WI), Nash Sutcliffe\\nEfficiency(NSE),relativeRMSE(RRMSE),andPearson’scorrelation\\ncoefficient (r). The tables show the number of iterations (M)for\\nthe proposed and competing methods, the best solution at iteration\\njis denoted by (S⋆\\nj)and the size of that solution (size(S⋆\\nj))for\\nthat iteration. A total of Npoints were used for the evaluation.\\nThe predicted values are denoted by ̂Vn, while observed values are\\ndenoted by Vn.\\n3.3 Feature selection results\\nThe evaluation of the feature selection results are discussed in\\nthis section. Table\\xa05 presents the results achieved by the proposed\\nfeature selection compared to other feature selection methods. In\\nthis table, the average error metric indicates the misclassification\\nrate of the feature selection algorithm. In this case, the average\\nerror rate is 0.406, which suggests that the algorithm is correct\\nabout 59.4% of the time (since the error rate is 1 - accuracy). This\\nFrontiers in Energy Research 08 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 8}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nTABLE2 Configurationparametersoftheoptimizationalgorithms.\\nAlgorithm Parameter Value\\nGAImmanuel and Chakraborty (2019) Cross over 0.9\\nMutation ratio 0.1\\nSelection mechanism Roulette wheel\\nNumber of iterations 80\\nNumber of agents 10\\nDTOTakieldeen\\xa0et\\xa0al. (2022) Iterations 100\\nNumber of runs 30\\nExploration percentage 70\\nPSOAwange\\xa0et\\xa0al. (2018) Acceleration constants (2, 12)\\nNumber of particles 10\\nNumber of iterations 80\\nGWOMirjalili\\xa0et\\xa0al. (2014) Number of wolves 10\\nNumber of iterations 80\\nWOAMirjalili and Lewis (2016) Number of whales 10\\nNumber of iterations 80\\nFAAriyaratne and Fernando (2023) Number of fireflies 10\\nTABLE3 Themetricsusedinevaluatingthefeatureselectionresults.\\nMetric Formula\\nBest fitness minM\\ni=1S⋆\\ni\\nWorst fitness maxM\\ni=1S⋆\\ni\\nAverage error1\\nM∑M\\nj=11\\nN∑N\\ni=1mse(̂Vi−Vi)\\nAverage fitness1\\nM∑M\\ni=1S⋆\\ni\\nAverage fitness size1\\nM∑M\\ni=1size(S⋆\\ni)\\nStandard deviation √1\\nM−1∑M\\ni=1(S⋆\\ni−Mean)2\\nmetric can be useful in evaluating the overall performance of the\\nfeature selection algorithm. The average select size metric indicates\\nthe average percentage of the selected feature set. In this case, the\\naveragepercentageis0.379,whichsuggeststhatthealgorithmselects\\nfewer features when compared to the other methods in the table.\\nThis can indicate that the algorithm is effective at identifying a small\\nsubset of important predictors. The Average Fitness metric indicates\\nthe average fitness or quality of the selected feature set. Fitness\\nmeasureshowwelltheselectedfeaturespredicttheoutcomevariable\\n(in this case, wind speed). In this case, the average fitness is 0.489,\\nwhich suggests that the selected feature set performs reasonably\\nwell at predicting wind speed. The Best Fitness metric indicates the\\nbest fitness or quality of the selected feature set. In this case, the\\nbest fitness is 0.391, which suggests that the algorithm was able\\nto identify a subset of features that performs well at predicting\\nwind speed. The Worst Fitness metric indicates the worst fitness\\nor quality of the selected feature set. In this case, the worst fitnessTABLE4 Themetricsusedinevaluatingthewindspeedpredictionresults.\\nMetric Formula\\nRMSE √1\\nN∑N\\nn=1(̂Vn−Vn)2\\nRRMSERMSE\\n∑N\\nn=1̂Vn×100\\nMAE1\\nN∑N\\nn=1|̂Vn−Vn|\\nMBE1\\nN∑N\\nn=1(̂Vn−Vn)\\nNSE 1−∑N\\nn=1(Vn−̂Vn)2\\n∑N\\nn=1(Vn−̄̂Vn)2\\nWI 1−∑N\\nn=1|̂Vn−Vn|\\n∑N\\nn=1|Vn−̄Vn|+|̂Vn−̄̂Vn|\\nR21−∑N\\nn=1(Vn−̂Vn)2\\n∑N\\nn=1(∑N\\nn=1Vn)−Vn)2\\nr∑N\\nn=1(̂Vn−̄̂Vn)(Vn−̄Vn)\\n√(∑N\\nn=1(̂Vn−̄̂Vn)2)(∑N\\nn=1(Vn−̄Vn)2)\\nis the same as the average fitness (0.489), which suggests that no\\nselected feature sets perform significantly worse than average. The\\nStd.Thefitnessmetricindicatesthefitnessscores’standarddeviation\\nfor the selected feature sets. A larger standard deviation indicates\\nthat the fitness scores are more spread out, while a smaller standard\\ndeviation indicates that they are more tightly clustered. In this case,\\nthestandarddeviationis0.311,whichsuggeststhatthefitnessscores\\nare moderately spread out. These results suggest that the proposed\\nfeature selection algorithm (bGADTO) can identify a small subset\\nof important predictors that perform reasonably well at predicting\\nwind speed. While the average error rate of 0.406 suggests that the\\nalgorithm may not be highly accurate, the fact that the best fitness\\nscore is relatively low (0.391) suggests that there are selected feature\\nsets that perform quite well. The moderate standard deviation of\\nFrontiers in Energy Research 09 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 9}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nTABLE5 Evaluationresultsoftheproposedfeatureselectionmethodandothermethods.\\nbGADTO bDTO bGWO bPSO bWAO bFA bGA\\nAverage error 0.406 0.482 0.443 0.477 0.477 0.475 0.457\\nAverage Select size 0.379 0.712 0.579 0.579 0.742 0.613 0.521\\nAverage Fitness 0.489 0.513 0.505 0.504 0.511 0.555 0.517\\nBest Fitness 0.391 0.467 0.425 0.484 0.475 0.474 0.420\\nWorst Fitness 0.489 0.577 0.492 0.552 0.552 0.572 0.535\\nStd. Fitness 0.311 0.334 0.316 0.315 0.318 0.352 0.318\\nfitness scores suggests that the algorithm can identify multiple\\nfeature sets that perform reasonably well.\\nOn the other hand, the study of the statistical difference of\\nthe proposed feature selection algorithm compared to different\\nalgorithms is performed using the analysis of variance (ANOVA)\\ntest as shown in Table\\xa06 . In this table, [Treatment, SS] = 0.04658\\nrepresents the sum of squares for the treatment effect. It measures\\nthe variation in the response variable (e.g., wind speed) that is\\nexplained by the treatment (e.g., the feature selection algorithm).\\nIn this case, the sum of squares is 0.04658. [Treatment, DF] =\\n6 represents the degrees of freedom for the treatment effect. It\\nis calculated as the number of treatments minus one. In this\\ncase, there are six treatments, so the degrees of freedom are six\\nminus one or five. [Treatment, MS] = 0.007763: This represents the\\nmean square for the treatment effect. It is calculated by dividing\\nthe sum of squares by the degrees of freedom. In this case, the\\nmean square is 0.007763. [Treatment, F (6, 63)] = 144.8: This\\nrepresents the F-statistic for the treatment effect. It is calculated\\nby dividing the mean square for the treatment effect by the mean\\nsquare for the error term (which measures the variation in the\\nresponse variable that is not explained by the treatment). In this\\ncase, the F-statistic is 144.8. [Treatment, p-value] = “¡0.0001”:\\nThis represents the p-value for the treatment effect. It measures\\nthe probability of observing the F-statistic (or a more extreme\\nF-statistic) if the null hypothesis is true (i.e., if the treatment\\ndoes not affect the response variable). A p-value of less than 0.05\\nis typically considered statistically significant, which means we\\ncan reject the null hypothesis and conclude that the treatment\\nsignificantly affects the response variable. These results suggest that\\nthe feature selection algorithm significantly affects the prediction\\nof wind speed. The F-statistic of 144.8 and the p-value of less than\\n0.0001 indicate that the variation in wind speed explained by the\\nfeature selection algorithm is significantly greater than the variation\\nnot explained by the algorithm. The relatively large F-statistic and\\nlowp-valuesuggeststhattheeffect ofthefeatureselectionalgorithm\\nis quite strong. The results also provide some information about the\\nspecific treatments that were used in the experiment. The fact that\\nthere are six degrees of freedom for the treatment effect suggests\\nthat there were six different treatments (e.g., six different feature\\nselection algorithms or six different parameter settings for a single\\nalgorithm). The mean square for the treatment effect (0.007763)\\nindicates that there is relatively little variation between the different\\ntreatments, while the sum of squares (0.04658) indicates that there\\nis a significant amount of variation overall. This suggests that someTABLE6 ANOVAtestappliedtothefeatureselectionevaluationresults.\\nSS DFMS F(DFn,DFd) p-value\\nTreatment 0.04658 6 0.007763 F (6, 63) = 144.8 p<0.0001\\nResidual 0.003379 63 0.00005363\\nTotal 0.04996 69\\ntreatments may be more effective than others but that no clear\\nwinner consistently outperforms the others. Overall, these ANOVA\\nresults provide valuable information about the effectiveness of a\\nfeature selection algorithm for wind speed prediction. They suggest\\nthat the algorithm significantly affects wind speed prediction and\\nthat there may be some variability in the performance of different\\ntreatments.\\nThe Wilcoxon test results are shown in Table\\xa07 . The Wilcoxon\\nsigned-rank test is a non-parametric statistical test used to compare\\ntwo related samples. In the context of feature selection for wind\\nspeed prediction, it can be used to determine whether a feature\\nselection algorithm significantly improves the accuracy of the\\npredictions. The theoretical median = 0 refers to the expected\\nmedian of the distribution of the differences between the predicted\\nwind speed values with and without the feature selection algorithm.\\nA theoretical median of 0 suggests that there should be no\\nsignificant difference between the two sets of predictions. The\\nactual median = 0.406 refers to the observed median of the\\ndistribution of the differences between the predicted wind speed\\nvalues with and without the feature selection algorithm. An actual\\nmedian significantly different from the theoretical median suggests\\na significant difference between the two sets of predictions. The\\nnumber of values = 10 refers to the number of paired observations\\nused in the Wilcoxon signed-rank test. Each observation consists\\nof the difference between the predicted wind speed values with\\nand without the feature selection algorithm. Sum of signed ranks\\n(W) = 55 refers to the sum of the signed ranks of the differences\\nbetween the predicted wind speed values with and without the\\nfeature selection algorithm. The signed ranks are calculated by\\nranking the absolute values of the differences and then assigning\\npositive or negative signs based on whether the original difference\\nwas positive or negative. A larger sum of signed ranks suggests a\\nsignificant difference between the two sets of predictions. The sum\\nof positive ranks = 55 refers to the sum of the ranks assigned to\\nthe positive differences between the predicted wind speed values\\nFrontiers in Energy Research 10 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 10}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nTABLE7 WilcoxontestappliedtotheRMSEresultsoffeatureselectionmethods.\\nbGADTO bDTO bGWO bPSO bWAO bFA bGA\\nTheoretical median 0 0 0 0 0 0 0\\nActual median 0.406 0.443 0.482 0.477 0.477 0.475 0.457\\nNumber of values 10 10 10 10 10 10 10\\nSum of±ranks (W) 55 55 55 55 55 55 55\\nSum of +ve ranks 55 55 55 55 55 55 55\\nSum of −ve ranks 0 0 0 0 0 0 0\\np-value 0.002 0.002 0.002 0.002 0.002 0.002 0.002\\nDiscrepancy 0.406 0.443 0.482 0.477 0.477 0.475 0.457\\nFIGURE7\\nAnalysis plots of the results achieved by the proposed feature selection algorithm.\\nwith and without the feature selection algorithm. A larger sum of\\npositiverankssuggeststhatthepredictionswiththefeatureselection\\nalgorithm are consistently better than those without the algorithm.\\nThesumofnegativeranks=0referstothesumoftheranksassigned\\nto the negative differences between the predicted wind speed values\\nwith and without the feature selection algorithm. A sum of negative\\nranks of 0 suggests that the predictions without the feature selection\\nalgorithm are not consistently better than the predictions with the\\nalgorithm. p-value = 0.002 refers to the probability of obtaining a\\ntest statistic as extreme as the observed one (i.e., a sum of signed\\nranks of 55) under the null hypothesis that there is no significant\\ndifference between the two sets of predictions. A p-value of less than\\n0.05 is typically considered statistically significant, which means wecanrejectthenullhypothesisandconcludethatthefeatureselection\\nalgorithm significantly improves the accuracy of the wind speed\\npredictions. Discrepancy = 0.406 refers to the median difference\\nbetween the predicted wind speed values with and without the\\nfeature selection algorithm. A larger discrepancy suggests that the\\npredictions with the feature selection algorithm are consistently\\nbetter than those without the algorithm. These Wilcoxon signed\\nranktestresultsprovideevidencethatthefeatureselectionalgorithm\\nsignificantly improves the accuracy of the wind speed predictions.\\nThe observed median of 0.406 and the p-value of 0.002 suggest that\\nthe difference between the predicted wind speed values with and\\nwithout the feature selection algorithm is significant. The sum of\\nsigned ranks of 55 and positive ranks of 55 further support this\\nFrontiers in Energy Research 11 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 11}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nconclusion, indicating that the predictions with the feature selection\\nalgorithm are consistently better than those without the algorithm.\\nOverall, these results provide valuable insights into the effectiveness\\nof the feature selection algorithm for wind speed prediction.\\nFigure\\xa08 shows the average error of the feature selection\\nalgorithms when compared with the proposed feature selection\\nmethod. In this figure, the proposed method achieves the smallest\\naverage error, which indicates the superiority of the proposed\\nmethod compared to the other six methods. In addition, several\\nplotscanbeusedtovisualizetheresultsoftheANOVAtest,asshown\\ninFigure\\xa07 . The residual, homoscedasticity, QQ, and heatmap plots\\narecommonlyused.Theresidualplotisascatterplotofthepredicted\\nvalues versus the residuals (i.e., the differences between the actual\\nand predicted values). A good residual plot should show no clear\\npattern, indicating that the model makes unbiased predictions. In\\nthe context of wind speed prediction, a residual plot can be used to\\nevaluatetheaccuracyofamodel’spredictions.Thehomoscedasticity\\nplot is a scatter plot of the predicted values versus the residuals.\\nThe residuals are plotted on the y-axis, and the predicted values\\nare on the x-axis. A good homoscedasticity plot should show\\nno clear pattern, indicating that the variance of the residuals is\\nconstant across all predicted values. In wind speed prediction, a\\nhomoscedasticity plot can be used to evaluate the stability of a\\nmodel’s predictions. The QQ plot is a scatter plot of the quantiles\\nof the residuals versus the quantiles of a normal distribution. A\\ngood QQ plot should show the residuals following a straight line,\\nindicating that they are normally distributed. In the context of wind\\nspeed prediction, a QQ plot can be used to evaluate the normality of\\ntheresiduals.Theheatmapplotisavisualrepresentationofamatrix,\\nwhere colors represent the values of the matrix. In wind speed\\nprediction, a heatmap plot can be used to visualize the correlation\\nbetween different features or models. By analyzing these plots,\\nwe can identify any patterns or outliers that may be affecting the\\naccuracy of our wind speed prediction model. We can also use these\\nplotstocomparetheaccuracyofdifferentmodelsorfeatureselection\\nmethods. For example, suppose that one feature selection method\\nconsistently produces a lower residual variance than another. In that\\ncase, we can conclude that it is a better method for our wind speed\\nprediction task.\\nFIGURE8\\nThe average error of the results achieved by the proposed feature\\nselection algorithm.\\nTABLE8 Resultsofevaluatingwindspeedforecastingusingthebaseline\\nBiLSTMpredictionmodel.\\nRMSEMAEMBErR2RRMSE NSEWI\\nBiLSTM 0.012 0.007 −0.001 0.998 0.997 2.251 0.997 0.981\\n3.4 Wind speed forecasting results\\nThe evaluation of the forecasting results of the wind speed using\\nthe baseline BiLSTM prediction model are shown in Table\\xa08 . In\\nthis table, the RMSE measures how far off the model’s predictions\\nare from the true values, on average. An RMSE of 0.012 means\\nthat the average difference between the model’s predictions and\\nthe true values is 0.012 units of wind speed. This can be useful in\\nassessingtheoverallaccuracyofthemodel.MAEisanothermeasure\\nof the model’s prediction accuracy. It is similar to RMSE but looks\\nat the absolute difference between the model’s predictions and the\\ntrue values rather than the squared difference. An MAE of 0.007\\nmeansthat,onaverage,themodel’spredictionsareoffby0.007units\\nof wind speed. MBE is also a measure of the overall bias in the\\nTABLE9 StatisticalanalysisoftheRMSEofthepredictionresultsachievedusingtheproposedGADTO-BiLSTMcomparedtoothermethods.\\nGADTO-BiLSTM DTO-BiLSTM GA-BiLSTM GWO-BiLSTM PSO-BiLSTM\\nNumber of values 10 10 10 10 10\\nMinimum 0.00032 0.00067 0.00071 0.00087 0.00088\\nMaximum 0.00048 0.00071 0.00076 0.00090 0.00095\\nRange 0.00016 0.00004 0.00005 0.00002 0.00007\\nMean 0.00046 0.00068 0.00072 0.00088 0.00089\\nStd. Deviation 0.00005 0.00001 0.00002 0.00001 0.00002\\nStd. Error of Mean 0.00002 0.00000 0.00001 0.00000 0.00001\\nHarmonic mean 0.00045 0.00068 0.00072 0.00088 0.00089\\nSkewness −3.148 1.897 1.493 1.886 1.936\\nKurtosis 9.934 2.224 0.7724 2.149 2.492\\nFrontiers in Energy Research 12 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 12}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nTABLE10 ANOVAtestresultswhenappliedtothewindspeedpredictions\\nachievedbytheproposedandthecompetingalgorithms.\\nSS DFMS F(DFn,DFd) p-value\\nTreatment 0.00000125 4 3.126E-07 F (4, 45) = 420.8 P¡0.0001\\nResidual 3.343E-08 45 7.43E-10\\nTotal 0.000001284 49\\nmodel’s predictions. It tells us whether the model overestimates or\\nunderestimatesthetruevalues.Inthiscase,anMBEof−0.001means\\nthat, on average, the model’s predictions are lower than the true\\nvalues.Thercoefficientmeasuresthelinearrelationshipbetweenthe\\nmodel’s predictions and the true values. A value of 0.998 suggests\\na strong positive linear relationship between the two, which is a\\ngood sign\\xa0for the model’s predictive power. R2measures how much\\nvariation in the wind speed data can be explained by the model’s\\npredictions.Avalueof0.997meansthat99.7%ofthevariationinthe\\ndata can be accounted for by the model, which is another indication\\nthat the model is performing well. The RRMSE is a normalized\\nversionoftheRMSEthatconsiderstherangeofthewindspeeddata.\\nA value of 2.251 means that the RMSE is 2.251% of the range of the\\ndata, which can help to put the RMSE in context and compare it to\\nother models. The NSE measures the model’s predictive efficiency\\nrelative to the mean of the observed data. A value of 0.997 suggests\\nthat the model performs nearly as well as the “perfect” model,\\nwhich would have an NSE of 1. The WI is another measure of the\\nmodel’spredictiveefficiency.Avalueof0.981meansthatthemodel’s\\npredictions are promising, with only 1.9% error compared to the\\naverage absolute error of the observations. Although these results\\nare promising, but they still need more improvements to boost the\\nprediction efficiency. This motivate the authors of this paper to\\ndevelop new optimization algorithms to boost the performance of\\nthe BiLSTM prediction model to achieve more efficient results.\\nThe statistical analysis of the results is shown in Table\\xa09 .\\nTen different scenarios were used to assess the accuracy of\\nwind speed forecasts and generate these findings. The proposed\\napproach (GADTO + BiLSTM) has this table’s lowest minimum and\\nmaximum error values. Furthermore, the skewness of a distribution\\nofpredictedwindspeedsquantifiesthisasymmetry.Thedistributionis substantially skewed to the left (negatively skewed), skewness\\nof −3.148. This indicates that it is more likely than not that the\\npredicted wind speeds will be lower than the actual values. Kurtosis\\nquantifies how “peaky” the distribution of predicted wind speeds\\nis. With a kurtosis of 9.934, the distribution is likely to be highly\\npeaked,withamorepronouncedpeakthaninanormaldistribution.\\nThis may suggest that the wind speed estimates are less dispersed\\naround the mean. The harmonic mean is a special kind of average\\nthat differs from the more typical arithmetic mean in its calculation\\nmethod. The average reciprocal of the predicted wind speeds is\\n0.00045,whichgivesustheharmonicmeanofpredictedwindspeeds\\nof 0.00045. Calculating average speeds over a specified period is\\none example of an application where this might be helpful. The\\nmedian indicates the average value of a set of estimates for wind\\nspeed. Half of the estimates for the wind speed will be lower than\\nthe median of 0.00045, while the other half will be higher. This\\ncan be helpful when the predicted wind speed distribution has\\noutliers that could throw off the average. Together, these measures\\nof quality assurance point to a wind speed prediction distribution\\nthat is substantially skewed to the left and has a stronger peak than\\na normal distribution would. According to the harmonic mean and\\nmedian, the average predicted wind speed is roughly 0.00045. With\\nthese measures, we may learn more about the dispersion and central\\ntendency of the predicted wind speeds and how they compare to\\nthe actual values. These findings validate the proposed method’s\\nadvantage over four competing strategies. In addition, the RMSE\\nresults presents in this table significantly outperform the results\\nachieved by the baseline BiLSTM, which proves the effectiveness of\\nthe proposed methodology.\\nThe results of the ANOVA test, when applied to the results of\\nthe wind speed prediction using the optimized BiLSTM prediction\\nmodel, are shown in Table\\xa010 . The F-statistic is a test statistic\\nused to compare the degree to which one group (treatment)\\nvaries from another. Our F-statistic is a very respectable\\xa0420.8.\\nWithin-group degrees of freedom are indicated by the number\\n(45), while between-group degrees of freedom is measured by the\\nnumber (45). The number of groups or treatments and sample\\nsize determines the degrees of freedom. If the F-statistic is large,\\nthen the variability between groups is larger than the variability\\nwithin groups. This p-value is related to the F-statistic (P0.0001).\\nUndertheassumptionthatthenullhypothesisiscorrect,the p-value\\nTABLE11 Wilcoxontestresultswhenappliedtothewindspeedpredictionsachievedbytheproposedandthecompetingalgorithms.\\nGADTO-BiLSTM DTO-BiLSTM GA-BiLSTM GWO-BiLSTM PSO-BiLSTM\\nTheor. median 0 0 0 0 0\\nActual median 0.00048 0.00067 0.00071 0.00087 0.00088\\nNum. of values 10 10 10 10 10\\nSum of±ranks 55 55 55 55 55\\nSum of +ve ranks 55 55 55 55 55\\nSum of −ve ranks 0 0 0 0 0\\np-value 0.002 0.002 0.002 0.002 0.002\\nSignificant? Yes Yes Yes Yes Yes\\nDiscrepancy 0.00048 0.00067 0.00071 0.00087 0.00088\\nFrontiers in Energy Research 13 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 13}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nFIGURE9\\nThe RMSE values measured from the results achieved by the proposed\\nand other compared optimization methods when applied to optimize\\nthe BiLSTM forecasting model.\\nindicates the likelihood of witnessing the calculated test statistic\\n(i.e., no differences between the groups). For statistical significance,\\nthep-value must be less than 0.05, indicating substantial evidence\\nagainst the null hypothesis. The p-value, in this case, is less than\\n0.0001, suggesting statistically significant variations in predicted\\nwind speed across the treatment groups. Based on these findings,\\nthe predicted wind speeds vary significantly between the various\\nFIGURE10\\nHistogram of the RMSE achieved by the proposed and other methods\\nfor optimization methods when applied to optimize the BiLSTM\\nforecasting model.\\ngroups or treatments. A high degree of variability between groups\\nrelative to within groups is shown by the F-statistic of 420.8 and the\\np-value of less than 0.0001, providing strong evidence against the\\nnull hypothesis that there are no differences between the groups.\\nTable\\xa011 presents the Wilcoxon signed rank test results. Similar\\nto the ANOVA test, the Wilcoxon test is applied to the prediction\\nresults using the proposed optimized BiLSTM and the other\\noptimization methods. The results in this table show the statistical\\ndifference between the proposed and other methods.\\nThe RMSE values calculated for the achieved results of the\\nproposed GADTO-BiLSTM method and other methods are shown\\nFIGURE11\\nVisualizing the results of the achieved prediction results achieved by the optimized BiLSTM forecasting model.\\nFrontiers in Energy Research 14 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 14}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\ninFigure\\xa09 . In this figure, the proposed method achieved the\\nminimum value of RMSE, which emphasizes the superiority of the\\nproposed method.\\nThis experiment helps study the stability of the proposed\\napproach for various test cases. The histogram of the RMSE values\\nis depicted in Figure\\xa010 . This histogram shows the number of\\nexperiments and the RMSE value achieved for each experiment.\\nThe figure shows that most test cases achieved the minimum RMSE\\nvalues, reflecting the proposed method’s stability. In addition, the\\nplots shown in Figure\\xa011 are used to visualize the prediction results\\nusing the proposed GADTO-BiLSTM. The residual error shown\\nin residual and homoscedasticity plots is minimal in these plots,\\nemphasizing the robustness of the prediction results. In addition,\\nthe QQ plot and heatmap show accurate results and thus confirm\\nthe proposed methodology’s effectiveness.\\n3.5 Discussion\\nTo robustly handle the wind speed forecasting process, we\\nevaluated two algorithms in this section. The first algorithm was\\ndeveloped to optimize the BiLSTM prediction model to improve\\nprediction results. Whereas the second algorithm was developed\\nto address the feature selection process to guarantee to perform\\nthe prediction in terms of the best set of features. This algorithm\\nproposed to fill the gap of low accuracy of the current prediction\\nmodels. In this section, two sets of experiments were conducted.\\nThe first experiment targeted evaluating the performance of the\\nproposed feature selection algorithm, bGADTO. To prove the\\neffectiveness of the proposed algorithm, six recent feature selection\\nalgorithms were included in the conducted experiments. The results\\nproved the superiority of the proposed methodology based on a\\nstandardsetofevaluationcriteria.Ontheotherhand,thesecondset\\nof experiments was conducted to evaluate the performance of the\\noptimized BiLSTM model, which is optimized using the proposed\\nGADTO algorithm. Four other approaches were employed in the\\nconducted experiment to show the effectiveness and superiority of\\nthe proposed model. The presented results in this section and the\\nvisualplotsconfirmedtheproposedmethodology’seffectivenessand\\nefficiencyinpredictingwindspeedwiththesmallesterror.Themain\\nadvantage of this work is that the proposed optimization algorithm\\ncould improve the performance of the BiLSTM forecasting model\\nas one of the most common approach in predicting time series\\ndata. The results showed that the improvements achieved by\\nthe proposed optimization algorithm are superior to the other\\noptimization algorithms. The limitations of this algorithm are not\\nclearly identified in this work,but it is planned touse this algorithm,\\nin the future work, with more prediction tasks of varying sizes to\\nidentify its limitations.\\n4 Conclusion\\nOne of the essential factors in the efficient distribution of\\nelectricity is the quality of the energy management system, which\\nhas been the focus of recent studies in the field of renewable energy.Predicting how fast the wind will blow is another crucial factor. This\\npaper proposes a new approach for robust wind speed prediction\\nbasedonBiLSTMandanoveloptimizationalgorithm.Theproposed\\noptimization algorithm is based on GA and DTO and is referred\\nto as the GADTO algorithm. This optimization algorithm is used\\nto optimize the parameters of BiLSTM prediction model. GADTO-\\nBiLSTM denotes the optimized model. In addition, the binary\\nversion of this optimization algorithm is used for selecting the most\\nsignificant set of features to boost the prediction accuracy. To prove\\nthe superiority of the proposed method, four other optimization\\nmethods are included in the conducted experiments. The proposed\\napproach is evaluated in terms of a freely available Kaggle dataset\\nused as the benchmark dataset. The results of the proposed feature\\nselection method and the proposed optimized model are evaluated\\nusing a set of criteria and statistical tests compared to other\\ncompeting methods. These tests’ results emphasized the proposed\\nmethod’s statistical difference and superiority in predicting wind\\nspeed in the adopted dataset. Based on the adopted evaluation\\ncriteria, the achieved RMSE is (0.012), MAE is (0.007), MBE is\\n(−0.001), r is (0.998), R2is (2.251), NSE is (0.997), and WI is\\n(0.981). These results confirm the effectiveness of the proposed\\nmethodology in wind speed prediction. The future perspective of\\nthis work includes evaluating the proposed methodology on a larger\\ndataset and including more optimization methods and prediction\\nmodels to confirm the superiority of the proposed methodology.\\nData availability statement\\nTheoriginalcontributionspresentedinthestudyareincludedin\\nthearticle/supplementarymaterial,furtherinquiriescanbedirected\\nto the corresponding author.\\nAuthor contributions\\nConceptualization, ME; methodology, AA and E-SE-K;\\nsoftware, E-SE-K and ME; validation, DK; formal analysis, AA and\\nDK; investigation, E-SE-K and ME; writing—original draft, ME,\\nAA, AI, and DK; writing—review & editing, AA, AI, and E-SE-\\nK; visualization, AA and AI; project administration, E-SE-K, and\\nAll authors contributed to the article and approved the submitted\\nversion.\\nFunding\\nPrincess Nourah bint Abdulrahman University Researchers\\nSupportingProjectnumber(PNURSP2023R308),PrincessNourah\\nbint Abdulrahman University, Riyadh, Saudi Arabia.\\nConflict of interest\\nThe authors declare that the research was conducted in the\\nabsence of any commercial or financial relationships that could be\\nconstrued as a potential conflict of interest.\\nFrontiers in Energy Research 15 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 15}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nPublisher’s note\\nAll claims expressed in this article are solely those of the\\nauthors and do not necessarily represent those of their affiliatedorganizations, or those of the publisher, the editors and the\\nreviewers.Anyproductthatmaybeevaluatedinthisarticle,orclaim\\nthatmaybemadebyitsmanufacturer,isnotguaranteedorendorsed\\nby the publisher.\\nReferences\\nAbdel Samee, N., El-Kenawy, M., Atteia, G., Jamjoom, M., Ibrahim, A., Abdelhamid,\\nA., et al. (2022). Metaheuristic optimization through deep learning classification of\\nCOVID-19 in chest X-ray images. Comput.Mater.Continua 73, 4193–4210.\\nAkhil, V., Wadhvani, R., Gyanchandani, M., and Kushwah, A. K. (2022). “Clustering-\\nbased hybrid approach for wind speed forecasting,” in Proceedings of data analytics\\nand management Editors D. Gupta, Z. Polkowski, A. Khanna, S. Bhattacharyya, and\\nO. Castillo (Singapore: Springer Nature), 587–598. doi: 10.1007/978-981-16-6289-8_49\\nAlbalawi, H., El-Shimy, M. E., AbdelMeguid, H., Kassem, A. M., and Zaid,\\nS. A. (2022). Analysis of a hybrid wind/photovoltaic energy system controlled\\nby brain emotional learning-based intelligent controller. Sustainability 14, 4775.\\ndoi:10.3390/su14084775\\nAriyaratne, M., and Fernando, T. (2023). A comprehensive review of the firefly\\nalgorithms for data clustering . Cham: Springer International Publishing, 217.\\ndoi:10.1007/978-3-031-09835-2_12\\nAwange, J. L., Paláncz, B., Lewis, R. H., and Völgyesi, L. (2018). “Particle swarm\\noptimization,” in Mathematical geosciences: Hybrid symbolic-numeric methods Editors\\nJ. L. Awange, B. Paláncz, R. H. Lewis, and L. Völgyesi (Cham: Springer International\\nPublishing). 167–184. doi: 10.1007/978-3-319-67371-4_6\\nBrown, B. G., Katz, R. W., and Murphy, A. H. (1984). Time series models to simulate\\nand forecast wind speed and wind power. J.Appl.MeteorologyClimatol. 23, 1184–1195.\\ndoi:10.1175/1520-0450(1984)023 ⟨1184:TSMTSA-2.0.CO;2\\nCadenas, E., Rivera, W., Campos-Amezcua, R., and Heard, C. (2016). Wind speed\\nprediction using a univariate ARIMA model and a multivariate NARX model. Energies\\n9, 109. doi: 10.3390/en9020109\\nCadenas, E., and Rivera, W. (2009). Short term wind speed forecasting in La\\nVenta, Oaxaca, México, using artificial neural networks. Renew. Energy 34, 274–278.\\ndoi:10.1016/j.renene.2008.03.014\\nCarvalho, D., Rocha, A., Gómez-Gesteira, M., and Santos, C. (2012). A sensitivity\\nstudy of the WRF model in wind simulation for an area of high wind energy. Environ.\\nModel.Softw. 33, 23–34. doi: 10.1016/j.envsoft.2012.01.019\\nChen,G.,Tang,B.,Zeng,X.,Zhou,P.,Kang,P.,andLong,H.(2022).Short-termwind\\nspeed forecasting based on long short-term memory and improved BP neural network.\\nInt.J.Electr.Power&EnergySyst. 134, 107365. doi: 10.1016/j.ijepes.2021.107365\\nDemolli, H., Dokuz, A. S., Ecemis, A., and Gokcek, M. (2019). Wind power\\nforecasting based on daily wind speed data using machine learning algorithms. Energy\\nConvers.Manag. 198, 111823. doi: 10.1016/j.enconman.2019.111823\\nDoaa, D. S., Karim, F. K., Alshetewi, S., Ibrahim, A., Abdelhamid, A. A., and\\nEl-kenawy, E. M. (2022). Optimized weighted ensemble using dipper throated\\noptimization algorithm in metamaterial antenna. Comput. Mater. Continua 73,\\n5771–5788. doi: 10.32604/cmc.2022.032229\\nDumitru, C.-D., and Gligor, A. (2017). Daily average wind energy\\nforecasting using artificial neural networks. Procedia Eng. 181, 829–836.\\ndoi:10.1016/j.proeng.2017.02.474\\nEl-kenawy, E.-S. M., Albalawi, F., Ward, S. A., Ghoneim, S. S. M., Eid,\\nM. M., Abdelhamid, A. A., et al. (2022a). Feature selection and classification\\nof transformer faults based on novel meta-heuristic algorithm. Mathematics 10.\\ndoi:10.3390/math10173144\\nEl-Kenawy, E.-S. M., Mirjalili, S., Abdelhamid, A. A., Ibrahim, A., Khodadadi, N.,\\nand Eid, M. M. (2022b). Meta-heuristic optimization and keystroke dynamics for\\nauthentication of smartphone users. Mathematics 10. doi: 10.3390/math10162912\\nEldali, F. A., Hansen, T. M., Suryanarayanan, S., and Chong, E. K. P. (2016).\\n“Employing ARIMA models to improve wind power forecasts: A case study in ercot,” in\\n2016 North American Power Symposium (NAPS), Denver, CO, USA, 18-20 September\\n2016. doi: 10.1109/NAPS.2016.7747861\\nFang, S., and Chiang, H. (2016). Improving supervised wind power forecasting\\nmodels using extended numerical weather variables and unlabelled data. IET Renew.\\nPowerGener. 10, 1616–1624. doi: 10.1049/iet-rpg.2016.0339\\nFathima, A. H., and Palanisamy, K. (2016). Energy storage systems for energy\\nmanagementofrenewablesindistributedgenerationsystems . London, UK: IntechOpen.\\ndoi:10.5772/62766\\nFedesoriano, F. (2022). Wind speed prediction dataset. [Online] Available at: https://\\nwww.kaggle.com/datasets/fedesoriano/wind-speed-prediction-dataset .[Accessed2023\\n01 01]Higashiyama, K., Fujimoto, Y., and Hayashi, Y. (2017). “Feature extraction of\\nnumerical weather prediction results toward reliable wind power prediction,” in 2017\\nIEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe),\\nTorino, Italy, September 26-29, 2017. doi: 10.1109/ISGTEurope.2017.8260216\\nHoolohan, V., Tomlin, A. S., and Cockerill, T. (2018). Improved near surface\\nwind speed predictions using Gaussian process regression combined with numerical\\nweather predictions and observed meteorological data. Renew.Energy 126, 1043–1054.\\ndoi:10.1016/j.renene.2018.04.019\\nImmanuel, S. D., and Chakraborty, U. K. (2019). “Genetic algorithm: An approach\\nonoptimization,”in2019InternationalConferenceonCommunicationandElectronics\\nSystems (ICCES), July 17-19,2019, 701. doi: 10.1109/ICCES45898.2019.9002372\\nKavasseri, R. G., and Seetharaman, K. (2009). Day-ahead wind speed\\nforecasting using f-ARIMA models. Renew. Energy 34, 1388–1393.\\ndoi:10.1016/j.renene.2008.09.006\\nKhafaga, D. S., Alhussan, A. A., El-Kenawy, E.-S. M., Ibrahim, A., Eid, M. M.,\\nand Abdelhamid, A. A. (2022). Solving optimization problems of metamaterial and\\ndouble t-shape antennas using advanced meta-heuristics algorithms. IEEE Access 10,\\n74449–74471. doi: 10.1109/ACCESS.2022.3190508\\nLi, C., Lin, S., Xu, F., Liu, D., and Liu, J. (2018). Short-term wind power\\nprediction based on data mining technology and improved support vector\\nmachine method: A case study in northwest China. J. Clean. Prod. 205, 909–922.\\ndoi:10.1016/j.jclepro.2018.09.143\\nLiu, H., Mi, X., and Li, Y. (2018). Smart deep learning based wind speed\\nprediction model using wavelet packet decomposition, convolutional neural network\\nand convolutional long short term memory network. Energy Convers. Manag. 166,\\n120–131. doi: 10.1016/j.enconman.2018.04.021\\nLiu, X., Lin, Z., and Feng, Z. (2021). Short-term offshore wind speed forecast\\nby seasonal ARIMA - a comparison against GRU and LSTM. Energy 227, 120492.\\ndoi:10.1016/j.energy.2021.120492\\nLópez, G., and Arboleya, P. (2022). Short-term wind speed forecasting over\\ncomplex terrain using linear regression models and multivariable LSTM and\\nNARX networks in the Andes Mountains, Ecuador. Renew. Energy 183, 351–368.\\ndoi:10.1016/j.renene.2021.10.070\\nMirjalili,S.,andLewis,A.(2016).Thewhaleoptimizationalgorithm. Adv.Eng.Softw.\\n95, 51–67. doi: 10.1016/j.advengsoft.2016.01.008\\nMirjalili, S., Mirjalili, S. M., and Lewis, A. (2014). Grey wolf optimizer. Adv. Eng.\\nSoftw.69, 46–61. doi: 10.1016/j.advengsoft.2013.12.007\\nPraveena, R., and Dhanalakshmi, K. (2018). “Wind power forecasting in short-term\\nusingFuzzyK-meansclusteringandneuralnetwork,”in2018InternationalConference\\non Intelligent Computing and Communication for Smart World (I2C2SW), Erode,\\nIndia,14thto15thDecember2018,336–339.doi: 10.1109/I2C2SW45816.2018.8997350\\nRajagopalan, S., and Santoso, S. (2009). “Wind power forecasting and error\\nanalysis using the autoregressive moving average modeling,” in 2009 IEEE Power\\n& Energy Society General Meeting, Calgary, Alberta, Canada, 26-30 July 2009, 1.\\ndoi:10.1109/PES.2009.5276019\\nSaeed, A., Li, C., Danish, M., Rubaiee, S., Tang, G., Gan, Z., et al. (2020). Hybrid\\nbidirectional LSTM model for short-term wind speed interval prediction. IEEE Access\\n8, 182283–182294. doi: 10.1109/ACCESS.2020.3027977\\nSami Khafaga, D., Ali Alhussan, A., El-kenawy, M., Ibrahim, A., Abd Elkhalik, H.,\\nEl-Mashad,Y.S.,etal.(2022).Improvedpredictionofmetamaterialantennabandwidth\\nusing adaptive optimization of LSTM. Comput.Mater.Continua 73, 865–881.\\nSami Khafaga, D., Ali Alhussan, A., El-kenawy, M., Takieldeen, E., Hassan, A. .,\\nM., Hegazy, T., et al. (2022). Meta-heuristics for feature selection and classification in\\ndiagnostic breast-cancer. Comput.Mater.Continua 73, 749–765.\\nSfetsos, A. (2002). A novel approach for the forecasting of mean hourly wind speed\\ntime series. Renew.Energy 27, 163–174. doi: 10.1016/S0960-1481(01)00193-8\\nShabbir, N., Kt, L., Jawad, M., Husev, O., Ur Rehman, A., Abid Gardezi, A., et al.\\n(2022). Short-term wind energy forecasting using deep learning-based predictive\\nanalytics. Comput.Mater.Continua 72, 1017–1033. doi: 10.32604/cmc.2022.024576\\nShang, Z., He, Z., Chen, Y., Chen, Y., and Xu, M. (2022). Short-term wind speed\\nforecasting system based on multivariate time series and multi-objective optimization.\\nEnergy 238, 122024. doi: 10.1016/j.energy.2021.122024\\nFrontiers in Energy Research 16 frontiersin.org'),\n",
       " Document(metadata={'source': 'simple.pdf', 'page': 16}, page_content='Alhussan et\\xa0al. 10.3389/fenrg.2023.1172176\\nSun, F., and Jin, T. (2022). A hybrid approach to multi-step, short-term\\nwind speed forecasting using correlated features. Renew. Energy 186, 742–754.\\ndoi:10.1016/j.renene.2022.01.041\\nTakieldeen, E., El-kenawy, A. ., M., Hadwan, M., and Zaki, M. R. (2022). Dipper\\nthroated optimization algorithm for unconstrained function and feature selection.\\nComput.Mater.Continua 72, 1465–1481. doi: 10.32604/cmc.2022.026026\\nTorres, J. L., García, A., De Blas, M., and De Francisco, A. (2005). Forecast of hourly\\naverage wind speed with ARMA models in Navarre (Spain). Sol. Energy 79, 65–77.\\ndoi:10.1016/j.solener.2004.09.013\\nXiong, B., Lou, L., Meng, X., Wang, X., Ma, H., and Wang, Z. (2022).\\nShort-term wind power forecasting based on attention mechanism anddeep learning. Electr. Power Syst. Res. 206, 107776. doi: 10.1016/j.epsr.2022.\\n107776\\nYu,C.,Li,Y.,Bao,Y.,Tang,H.,andZhai,G.(2018).Anovelframeworkforwindspeed\\nprediction based on recurrent neural networks and support vector machine. Energy\\nConvers.Manag. 178, 137–145. doi: 10.1016/j.enconman.2018.10.008\\nYu, R., Liu, Z., Li, X., Lu, W., Ma, D., Yu, M., et al. (2019). Scene learning: Deep\\nconvolutional networks for wind power prediction by embedding turbines into grid\\nspace.Appl.Energy 238, 249–257. doi: 10.1016/j.apenergy.2019.01.010\\nZeng, P., Sun, X., and Farnham, D. J. (2020). Skillful statistical models to predict\\nseasonal wind speed and solar radiation in a Yangtze River estuary case study. Sci.Rep.\\n10, 8597. doi: 10.1038/s41598-020-65281-w\\nFrontiers in Energy Research 17 frontiersin.org')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"simple.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b52d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf39d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74598b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db8bfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f269ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a4111",
   "metadata": {},
   "source": [
    "### LowerCasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be20af95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b070d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"] = data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b28d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd53d48",
   "metadata": {},
   "source": [
    "### Removing html tags using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95add4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to My Websitebody{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}Welcome to My Awesome Website!This is a sample HTML document created for demonstration purposes.Feel free to explore and enjoy the content on this website.&copy; 2024 My Website. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Welcome to My Website</title><style>body{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}</style></head><body><header><h1>Welcome to My Awesome Website!</h1></header><main><p>This is a sample HTML document created for demonstration purposes.</p><p>Feel free to explore and enjoy the content on this website.</p></main><footer><p>&copy; 2024 My Website. All rights reserved.</p></footer></body></html>\"\"\"\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(\"\", text)\n",
    "\n",
    "print(remove_html_tags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed00c04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying remove_html_tags to review column of the data\n",
    "data[\"review\"] = data[\"review\"].apply(remove_html_tags)\n",
    "data[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473b1de",
   "metadata": {},
   "source": [
    "### Removing URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4130f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkout to my youtube channel \n",
      "please try to google here \n",
      "here is my url which is not secure \n"
     ]
    }
   ],
   "source": [
    "text1=\"checkout to my youtube channel https://www.youtube.com/watch?v=V9tJCQoBakA&t=225s\"\n",
    "text2=\"please try to google here www.google.com\"\n",
    "text3=\"here is my url which is not secure http://dummy.com\"\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(\"\", text)\n",
    "\n",
    "print(remove_url(text1))\n",
    "print(remove_url(text2))\n",
    "print(remove_url(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6267401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remvoing url \n",
    "data[\"review\"] = data[\"review\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2ed8c",
   "metadata": {},
   "source": [
    "### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47812cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is Venkt si Prmod my work is eating sleeping coding'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "exclude = string.punctuation\n",
    "punc=\"my name is Venk@t@ s@i Pr@mod!!!, my work is eating, sleeping, coding!!!!!.\"\n",
    "\n",
    "#method 1\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "#method 2\n",
    "def remove_punc2(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", exclude))\n",
    "\n",
    "remove_punc2(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14b338c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "data[\"review\"] = data[\"review\"].apply(remove_punc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87926500",
   "metadata": {},
   "source": [
    "### Removing chat words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6696fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please come here As Soon As Possible For your information'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words={\n",
    "   \" AFAIK\":\"As Far As I Know\",\n",
    "\"AFK\": \"Away From Keyboard\",\n",
    "\"ASAP\":\"As Soon As Possible\",\n",
    "\"BTW\":\"By The Way\",\n",
    "\"B4\":\"Before\",\n",
    "\"LAMO\":\"Laugh My A.. Off\",\n",
    "\"FYI\":\"For your information\"\n",
    "}\n",
    "\n",
    "chat = \"Please come here ASAP FYI\"\n",
    "def chat_conversion(text):\n",
    "    new_chat = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_chat.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_chat.append(word)\n",
    "    return \" \".join(new_chat)\n",
    "\n",
    "chat_conversion(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b356678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing chatwords\n",
    "data[\"review\"] = data[\"review\"].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3efdf32",
   "metadata": {},
   "source": [
    "### Spelling Correction using textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dbd6e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\saipr\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c4d60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The s my Crossing notebook else download the notebook\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Ths s my Procsing ntebook plse downlod ths ntebook\"\n",
    "txtblob = TextBlob(text)\n",
    "print(txtblob.correct().string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a05de3",
   "metadata": {},
   "source": [
    "### Removing the stop_words: \n",
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f09084a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\saipr\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saipr\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4143a178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saipr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "383ae97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b049b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pramod learning nlp nlp important\n"
     ]
    }
   ],
   "source": [
    "stp=\"i am pramod and we are learning nlp and here is why nlp are important\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  new_text=[]\n",
    "  for word in text.split():\n",
    "    if word not in stopwords.words(\"english\"):\n",
    "      new_text.append(word)\n",
    "  return \" \".join(new_text)\n",
    "\n",
    "print(remove_stopwords(stp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f6fd1",
   "metadata": {},
   "source": [
    "### Removing Emoji's and replacing the context of the Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cee026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e0f3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, :grinning_face_with_big_eyes::person_tipping_hand::grinning_face_with_big_eyes::person_tipping_hand: People\n",
      "•:bear::sunflower: Animals\n",
      "•:hamburger::tropical_drink: Food\n",
      "•:saxophone::soccer_ball: Activities\n",
      "•:oncoming_automobile::sunset: Travel\n",
      "•:light_bulb::party_popper: Objects\n",
      "•:sparkling_heart::input_symbols: Symbols\n",
      "•:crossed_flags::rainbow_flag: Flags\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "emoji_text = \"Hello,😊 how are you today? 🌟\"\n",
    "\n",
    "emoji_text2 = \"\"\"Hello, 😃💁😃💁 People\n",
    "•🐻🌻 Animals\n",
    "•🍔🍹 Food\n",
    "•🎷⚽ Activities\n",
    "•🚘🌇 Travel\n",
    "•💡🎉 Objects\n",
    "•💖🔣 Symbols\n",
    "•🎌🏳️‍🌈 Flags\"\"\"\n",
    "\n",
    "def remove_emoji(text):\n",
    "    clean_text=emoji.demojize(text)\n",
    "    return clean_text\n",
    "\n",
    "print(remove_emoji(emoji_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe509b1",
   "metadata": {},
   "source": [
    "### Word Tokenizing and Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89707608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saipr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80901c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'going', 'to', 'visit', 'delhi', 'tomorrow', 'evening']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "my_text=\"i am going to visit delhi tomorrow evening\"\n",
    "\n",
    "print(word_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d3cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI) is artificial intelligence capable of generating text, images or other data using generative models, often in response to prompts.',\n",
       " 'Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.',\n",
       " '[5][6]\\nImprovements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s.',\n",
       " 'These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.',\n",
       " '[3][10][11]']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus=\"\"\"Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI) is artificial intelligence capable of generating text, images or other data using generative models, often in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]\n",
    "Improvements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s. These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][10][11]\"\"\"\n",
    "\n",
    "sent_tokenize(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911a90d",
   "metadata": {},
   "source": [
    "### Stemming and Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a152eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "input_sentence = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "\n",
    "def stemming(text):\n",
    "    obj=PorterStemmer()\n",
    "\n",
    "    stem_word=[obj.stem(word) for word in text.split()]\n",
    "\n",
    "    return stem_word\n",
    "\n",
    "print(stemming(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cff14df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saipr\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b6ac8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Ikigai'\", 'by', 'Hector', 'Garcia', 'and', 'Francesc', 'Miralles', 'explores', 'the', 'Japanese', 'concept', 'of', 'finding', \"one's\", 'purpose', 'in', 'life', 'by', 'analyzing', 'the', 'habit', 'and', 'belief', 'of', 'the', \"world's\", 'longest-living', 'people.', 'Through', 'case', 'studies,', 'the', 'book', 'offer', 'practical', 'insight', 'on', 'how', 'to', 'live', 'a', 'more', 'fulfilling', 'life.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text=\"'Ikigai' by Hector Garcia and Francesc Miralles explores the Japanese concept of finding one's purpose in life by analyzing the habits and beliefs of the world's longest-living people. Through case studies, the book offers practical insights on how to live a more fulfilling life.\"\n",
    "\n",
    "def lammatization(text):\n",
    "    words=text.split()\n",
    "\n",
    "    lemmetizer=WordNetLemmatizer()\n",
    "\n",
    "    lemetized_word=[lemmetizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return lemetized_word\n",
    "\n",
    "print(lammatization(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
